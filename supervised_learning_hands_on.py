# -*- coding: utf-8 -*-
"""Supervised_learning_Hands On.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QC_xJbdnNysBJjC1Ww1QceM6Hyge7I3S

# **Bismillahhi Rehmani Rahim**

# **`Supervised Learning`**

*Supervised learning is a type of **Machine learning** where an algorithm is trained on a **labeled dataset**. The goal is for the algorithm to learn the **mapping** from inputs to outputs so it can predict the output for new, unseen data* **bold text**

### **`Process of Supervised Learning`**

1. **Data Collection**: Gather a dataset containing input-output pairs.

2. **Data Preprocessing**: Clean the data, handle missing values, normalize features, and split the data into training and testing sets.

3. **Model Selection**: Choose an appropriate model based on the problem type (**Regression or Classification**) and dataset characteristics.

4. **Training**: Use the training data to let the model learn the mapping from inputs to outputs by minimizing the loss function.

5. **Evaluation**: Assess the model's performance on the testing set using appropriate metrics (e.g., **Accuracy, Precision, Recall for Classification tasks, or Mean Squared Error for Regression tasks**).

6. **Hyperparameter Tuning**: Adjust hyperparameters (like Learning Rate, Regularization Parameters) to improve model performance.

7. **Deployment**: Use the trained model to make predictions on new, unseen data.

### ***`Important Formulas`***
"""

Image(filename='/content/drive/MyDrive/Screenshot (9).png')

Image(filename='/content/drive/MyDrive/Screenshot (10).png')

"""1.   Accuracy:
          When the dataset is balanced and you want a general measure of model performance.
2.   Precision:
          When the cost of false positives is high.
3.   Recall:
          When the cost of false negatives is high.
4.   F1 Score:
          When you need a balance between precision and recall, particularly with imbalanced datasets.
5.   Mean Squared Error (MSE):
          When you want to penalize larger errors more significantly.
6.   Root Mean Squared Error (RMSE):
          When you need an error metric that is in the same units as the target variable.
7.   Mean Absolute Error (MAE):
          When you want an error metric that treats all errors equally.
8.   R2 Score:
          When you need to measure the proportion of variance explained by the model.

## **`Classification or Regression`**

*We'll use the Iris dataset, a classic dataset in machine learning, to classify iris flowers into three species based on the length and width of their sepals and petals*

### ***`Import Libraries`***
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, confusion_matrix, r2_score
import matplotlib.pyplot as plt
import joblib
from IPython.display import Image
import pandas as pd
import numpy as np
import seaborn as sns

"""### ***`Load the Dataset`***

*Reading From File*
"""

path= '/content/drive/MyDrive/iris_data.csv'
data= pd.read_csv(path)

"""### ***`Data Processing`***

---


*View Data*
"""

data.head()

data.tail()

data.describe()

data.info()

sns.pairplot(data, hue="species")
plt.show()

"""### ***`Training Model`***

---


*Selecting Features and Labels*
"""

X = data[["petal_length", "petal_width", "sepal_length", "sepal_width"]]
y = data["species"]

"""*Spliting Data in Two Sets {**Training Set and Testing Set**}*"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""*Selecting a Model*"""

model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)

"""*Train on Data and Labels*"""

model.fit(X_train, y_train)

"""### ***`Evaluation`***

---

***`Prediction`***
"""

y_pred = model.predict(X_test)

df_y_test=pd.DataFrame(y_test)

df = df_y_test  # your DataFrame
df = df.reset_index(drop=True)
col ='species'  # index of the column

"""### ***`Printing Actual and Predicted Classes`***"""

for i in y_pred:
  print("Predicted Class: ", y_pred[i], " and Actual Class: ", df.loc[(i,col)])

"""### ***`Accuracy`***"""

accuracy = accuracy_score(y_test, y_pred)

print("Accuracy:", accuracy*100,"%")

"""### ***`Mean Squared Error`***"""

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

"""### **`R2 Score`**"""

r2 = r2_score(y_test, y_pred)
print("R2 Score:", mse)

"""### ***`Report`***"""

print(classification_report(y_test, y_pred))

"""### ***`Plot the Confusion Matrix`***"""

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.show()

"""### ***`Plot the Correlation Matrix`***"""

corr = data.corr()
sns.heatmap(corr, annot=True, cmap="coolwarm")
plt.show()

"""## **`Hyperparameter Tuning`**"""

from sklearn.model_selection import GridSearchCV

# Define the hyperparameters and their values for tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize the model
model_iris = RandomForestClassifier(random_state=42)

# Initialize Grid Search
grid_search = GridSearchCV(estimator=model_iris, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Perform Grid Search
grid_search.fit(X_train, y_train)

# Get the best parameters and the best model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

print(f"Best Parameters: {best_params}")

# Evaluate the tuned model
y_pred_iris_tuned = best_model.predict(X_test)
tuned_accuracy = accuracy_score(y_test, y_pred_iris_tuned)
print(f"Tuned Model Accuracy: {tuned_accuracy}")

"""## **`Deployment`**

***Save Model***
"""

joblib.dump(model, "iris_model.joblib")

"""***Load A Safed Model***"""

load_model= joblib.load('/content/drive/MyDrive/iris_model.joblib')

"""***Mapper Function For Class***"""

def mapp(cla):
  if cla == 0:
    clas= 'Iris Setosa'
    return clas
  elif cla ==1:
    clas= 'Iris Versicolor'
    return clas
  elif cla ==2:
    clas= 'Iris Virginica'
    return clas
  else:
    return None



"""***Input and Prediction on User Data***"""

def predict_class():
  petal_length = float(input("Enter petal length: "))
  petal_width = float(input("Enter petal width: "))
  sepal_length = float(input("Enter sepal length: "))
  sepal_width = float(input("Enter sepal width: "))

  new_data = np.array([[petal_length, petal_width, sepal_length, sepal_width]])

  predicted_class = ir.predict(new_data)[0]

  print(f"Predicted class: {mapp(predicted_class)}")

"""***Main Loop***"""

def main():
  load_model= joblib.load('/content/drive/MyDrive/iris_model.joblib')
  ir=load_model
  a=3
  while a>=0:
    predict_class()
    a=a-1

if __name__ == "__main__":
  main()

"""# **`UnSupervised Learning`**

*Unsupervised learning is a type of machine learning where the algorithm is trained on data that has not been labeled, classified, or categorized. Instead, the algorithm tries to learn the patterns and structure from the input data without explicit instructions on what to predict. The goal of unsupervised learning is to find hidden patterns or intrinsic structures in the data.*
"""

#!pip install scikit-learn==1.1.3

import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout
from tensorflow.keras.optimizers import Adam

# Load the dataset
boston = load_boston()
X = boston.data
y = boston.target

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Reshape the data to fit into the CNN
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Build the CNN model
model = Sequential()

model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(Dropout(0.5))

model.add(Conv1D(filters=32, kernel_size=2, activation='relu'))
model.add(Dropout(0.5))

model.add(Flatten())

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(1, activation='linear'))

# prompt: loss function cross intopy

model.compile(loss='mse', optimizer=Adam(learning_rate=0.01))

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='catogorical')

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)

# Evaluate the model
loss = model.evaluate(X_test, y_test)
print(f"Test Loss (Mean Squared Error): {loss}")

y_pred = model.predict(X_test)

for i in range(5):
  print("Actual Value: ",y_test[i]," and predicted value: ",y_pred[i])

